{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'MountainCar-v0'\n",
    "EPISODE = 1000 # Episode limitation\n",
    "# Hyper Parameters for DQN\n",
    "INITIAL_EPSILON = 0.5 # starting value of epsilon\n",
    "FINAL_EPSILON = 0.01 # final value of epsilon\n",
    "BATCH_SIZE = 32 # size of minibatch\n",
    "REPLAY_SIZE = 10000 # experience replay buffer size\n",
    "\n",
    "env = gym.make(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-0.54811972  0.        ] (2,) 2\n"
    }
   ],
   "source": [
    "state=env.reset()\n",
    "print(state,state.shape,len(state))\n",
    "for i in range(100):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):# experience replay buffer size\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def put(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def sample(self):\n",
    "        sample = random.sample(self.buffer, args.batch_size)\n",
    "        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))\n",
    "        states = np.array(states).reshape(args.batch_size, -1)\n",
    "        next_states = np.array(next_states).reshape(args.batch_size, -1)\n",
    "        return states, actions, rewards, next_states, done\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, state_dim, aciton_dim):\n",
    "        self.state_dim  = state_dim\n",
    "        self.action_dim = aciton_dim\n",
    "        self.epsilon = 1.0\n",
    "        \n",
    "        self.model = self.create_model()\n",
    "    \n",
    "    # def create_model(self):\n",
    "    #     model = tf.keras.Sequential([\n",
    "    #         tf.keras.Input((self.state_dim,)),\n",
    "    #         layers.Dense(32, activation='relu'),\n",
    "    #         layers.Dense(16, activation='relu'),\n",
    "    #         layers.Dense(self.action_dim)\n",
    "    #     ])\n",
    "    #     model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(0.005))\n",
    "    #     return model\n",
    "    \n",
    "    def predict(self, state):\n",
    "        return self.model.predict(state)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_dim])\n",
    "        self.epsilon *= 0.995\n",
    "        self.epsilon = max(EPISODE, FINAL_EPSILON)\n",
    "        q_value = self.predict(state)[0]\n",
    "        if np.random.random() < EPISODE:\n",
    "            return random.randint(0, self.action_dim-1)\n",
    "        return np.argmax(q_value)\n",
    "\n",
    "    def train(self, states, targets):\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "    # def __init__(self,state_size,action_size):\n",
    "    #     self.state_dim=state_size\n",
    "    #     self.action_dim=action_size\n",
    "    #     self.epsilon =EPISODE\n",
    "    #     self.create_network()\n",
    "\n",
    "    def create_model(self,lr=0.005):\n",
    "        model=tf.keras.Sequential()\n",
    "        model.add(tf.keras.Input((self.state_dim,)))\n",
    "        model.add(layers.Dense(32,activation='relu'))\n",
    "        model.add(layers.Dense(24,activation='relu'))\n",
    "        model.add(layers.Dense(self.action_dim,activation='linear'))\n",
    "        model.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(lr), metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return model\n",
    "        \n",
    "\n",
    "    # def action(self,state):\n",
    "    #     state = np.reshape(state, [1, self.state_dim])\n",
    "    #     self.epsilon *= args.eps_decay\n",
    "    #     self.epsilon = max(self.epsilon, FINAL_EPSILON)\n",
    "    #     q_value = self.predict(state)[0]\n",
    "    #     if np.random.random() < self.epsilon:\n",
    "    #         return random.randint(0, self.action_dim-1)\n",
    "    #     return np.argmax(q_value)\n",
    "    # def predict(self, state):\n",
    "    #     return self.model.predict(state)\n",
    "\n",
    "    # def train(self, states, targets):\n",
    "    #     self.model.fit(states, targets, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_13\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_39 (Dense)             (None, 32)                96        \n_________________________________________________________________\ndense_40 (Dense)             (None, 24)                792       \n_________________________________________________________________\ndense_41 (Dense)             (None, 3)                 75        \n=================================================================\nTotal params: 963\nTrainable params: 963\nNon-trainable params: 0\n_________________________________________________________________\nModel: \"sequential_14\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_42 (Dense)             (None, 32)                96        \n_________________________________________________________________\ndense_43 (Dense)             (None, 24)                792       \n_________________________________________________________________\ndense_44 (Dense)             (None, 3)                 75        \n=================================================================\nTotal params: 963\nTrainable params: 963\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "class Agent:\n",
    "    def __init__(self,env):\n",
    "        self.state_dim=env.observation_space.shape[0]\n",
    "        self.action_dim=env.action_space.n\n",
    "        self.memory = deque(maxlen=3000)\n",
    "        self.env=env\n",
    "        self.epsilon=1.0\n",
    "        self.epsilon_min=0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.train_batch=32\n",
    "        self.gamma=0.9\n",
    "\n",
    "        self.model = Network(self.state_dim, self.action_dim)\n",
    "        self.target_model = Network(self.state_dim, self.action_dim)\n",
    "        self.target_update()\n",
    "        self.buffer = ReplayBuffer()\n",
    "\n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "\n",
    "    def target_update(self):\n",
    "        weights = self.model.model.get_weights()\n",
    "        self.target_model.model.set_weights(weights)\n",
    "\n",
    "    def replay(self):\n",
    "        for _ in range(10):\n",
    "            states, actions, rewards, next_states, done = self.buffer.sample()\n",
    "            targets = self.target_model.predict(states)\n",
    "            next_q_values = self.target_model.predict(next_states)[range(args.batch_size),np.argmax(self.model.predict(next_states), axis=1)]\n",
    "            targets[range(args.batch_size), actions] = rewards + (1-done) * next_q_values * args.gamma\n",
    "            self.model.train(states, targets)\n",
    "    \n",
    "    def train(self, max_episodes=1000):\n",
    "        for ep in tqdm.trange(max_episodes):\n",
    "            done, total_reward = False, 0\n",
    "            state = self.env.reset()\n",
    "            while not done:\n",
    "                action = self.model.get_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.buffer.put(state, action, reward, next_state, done)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "            if self.buffer.size() >= REPLAY_SIZE:\n",
    "                self.replay()\n",
    "            self.target_update()\n",
    "agent=Agent(env)\n",
    "agent.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state=env.reset()\n",
    "for i in range(100):\n",
    "    env.render()\n",
    "    env.step(agent.target_model.get_action(state))\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594722302823",
   "display_name": "Python 3.7.7 64-bit ('tf2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}